{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BR4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjgmHdcfK7mI",
        "outputId": "7537cbb0-5035-4049-afb7-1c5a996c4956"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import heapq\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OFM0OemS9fz"
      },
      "source": [
        "podcasts_df = pd.read_csv('/content/drive/MyDrive/bookfinal/book_final_data.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "pPQ_7rHOUBW7",
        "outputId": "16e7be6b-a121-4dbe-df4d-7bd36dd4b939"
      },
      "source": [
        "podcasts_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>age</th>\n",
              "      <th>isbn</th>\n",
              "      <th>rating</th>\n",
              "      <th>book_title</th>\n",
              "      <th>book_author</th>\n",
              "      <th>year_of_publication</th>\n",
              "      <th>publisher</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Language</th>\n",
              "      <th>Category</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11676</td>\n",
              "      <td>34</td>\n",
              "      <td>0002005018</td>\n",
              "      <td>8</td>\n",
              "      <td>Clara Callan</td>\n",
              "      <td>Richard Bruce Wright</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>HarperFlamingo Canada</td>\n",
              "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
              "      <td>en</td>\n",
              "      <td>['Actresses']</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>116866</td>\n",
              "      <td>34</td>\n",
              "      <td>0002005018</td>\n",
              "      <td>9</td>\n",
              "      <td>Clara Callan</td>\n",
              "      <td>Richard Bruce Wright</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>HarperFlamingo Canada</td>\n",
              "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
              "      <td>en</td>\n",
              "      <td>['Actresses']</td>\n",
              "      <td>ottawa</td>\n",
              "      <td>,</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>110912</td>\n",
              "      <td>36</td>\n",
              "      <td>0374157065</td>\n",
              "      <td>10</td>\n",
              "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
              "      <td>Gina Bari Kolata</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>Farrar Straus Giroux</td>\n",
              "      <td>Describes the great flu epidemic of 1918, an o...</td>\n",
              "      <td>en</td>\n",
              "      <td>['Medical']</td>\n",
              "      <td>milpitas</td>\n",
              "      <td>california</td>\n",
              "      <td>usa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>197659</td>\n",
              "      <td>49</td>\n",
              "      <td>0374157065</td>\n",
              "      <td>9</td>\n",
              "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
              "      <td>Gina Bari Kolata</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>Farrar Straus Giroux</td>\n",
              "      <td>Describes the great flu epidemic of 1918, an o...</td>\n",
              "      <td>en</td>\n",
              "      <td>['Medical']</td>\n",
              "      <td>indiana</td>\n",
              "      <td>pennsylvania</td>\n",
              "      <td>usa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11676</td>\n",
              "      <td>34</td>\n",
              "      <td>0399135782</td>\n",
              "      <td>9</td>\n",
              "      <td>The Kitchen God's Wife</td>\n",
              "      <td>Amy Tan</td>\n",
              "      <td>1991.0</td>\n",
              "      <td>Putnam Pub Group</td>\n",
              "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
              "      <td>en</td>\n",
              "      <td>['Fiction']</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  age        isbn  ...      city         state  country\n",
              "0    11676   34  0002005018  ...   unknown       unknown  unknown\n",
              "1   116866   34  0002005018  ...    ottawa             ,  unknown\n",
              "2   110912   36  0374157065  ...  milpitas    california      usa\n",
              "3   197659   49  0374157065  ...   indiana  pennsylvania      usa\n",
              "4    11676   34  0399135782  ...   unknown       unknown  unknown\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL4mtVweYYfD",
        "outputId": "2245d62a-e99e-4a88-c38d-eb9ad0ccbf83"
      },
      "source": [
        "podcasts_df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 153428 entries, 0 to 153427\n",
            "Data columns (total 14 columns):\n",
            " #   Column               Non-Null Count   Dtype  \n",
            "---  ------               --------------   -----  \n",
            " 0   user_id              153428 non-null  int64  \n",
            " 1   age                  153428 non-null  int64  \n",
            " 2   isbn                 153428 non-null  object \n",
            " 3   rating               153428 non-null  int64  \n",
            " 4   book_title           153428 non-null  object \n",
            " 5   book_author          153428 non-null  object \n",
            " 6   year_of_publication  153428 non-null  float64\n",
            " 7   publisher            153428 non-null  object \n",
            " 8   Summary              153428 non-null  object \n",
            " 9   Language             153428 non-null  object \n",
            " 10  Category             153428 non-null  object \n",
            " 11  city                 153428 non-null  object \n",
            " 12  state                153428 non-null  object \n",
            " 13  country              153428 non-null  object \n",
            "dtypes: float64(1), int64(3), object(10)\n",
            "memory usage: 16.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OmXvbh_YzMi"
      },
      "source": [
        "\n",
        "podcasts_df.dropna(inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAsRiBf0B6Ut",
        "outputId": "d53ca27f-59d0-4b9b-f1e7-f005168d38c3"
      },
      "source": [
        "podcasts_df['book_title'].duplicated().sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Wq8xwACO2K"
      },
      "source": [
        "podcasts_df.drop_duplicates(subset =\"book_title\",\n",
        "                     keep = 'last', inplace = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqKngjiyCclE",
        "outputId": "1649dfe1-cdf8-48e5-ef35-b192475fe952"
      },
      "source": [
        "podcasts_df['book_title'].duplicated().sum()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "kB0iipetC5gj",
        "outputId": "47ede96e-f841-414f-f562-b6b5cf8aed87"
      },
      "source": [
        "podcasts_df.head(2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>age</th>\n",
              "      <th>isbn</th>\n",
              "      <th>rating</th>\n",
              "      <th>book_title</th>\n",
              "      <th>book_author</th>\n",
              "      <th>year_of_publication</th>\n",
              "      <th>publisher</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Language</th>\n",
              "      <th>Category</th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>116866</td>\n",
              "      <td>34</td>\n",
              "      <td>0002005018</td>\n",
              "      <td>9</td>\n",
              "      <td>Clara Callan</td>\n",
              "      <td>Richard Bruce Wright</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>HarperFlamingo Canada</td>\n",
              "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
              "      <td>en</td>\n",
              "      <td>['Actresses']</td>\n",
              "      <td>ottawa</td>\n",
              "      <td>,</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>197659</td>\n",
              "      <td>49</td>\n",
              "      <td>0374157065</td>\n",
              "      <td>9</td>\n",
              "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
              "      <td>Gina Bari Kolata</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>Farrar Straus Giroux</td>\n",
              "      <td>Describes the great flu epidemic of 1918, an o...</td>\n",
              "      <td>en</td>\n",
              "      <td>['Medical']</td>\n",
              "      <td>indiana</td>\n",
              "      <td>pennsylvania</td>\n",
              "      <td>usa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  age        isbn  ...     city         state  country\n",
              "1   116866   34  0002005018  ...   ottawa             ,  unknown\n",
              "3   197659   49  0374157065  ...  indiana  pennsylvania      usa\n",
              "\n",
              "[2 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZdS6-ceBrJu"
      },
      "source": [
        "podcasts_df=podcasts_df.iloc[0:9000].reset_index()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZwslEjdwXI1k",
        "outputId": "b1b10a08-698b-46fc-f19b-3926a5bd4b1e"
      },
      "source": [
        "podcasts_df['text'] = podcasts_df[['book_title', 'book_author', 'publisher', 'Summary','Category']].apply(lambda x: \" \".join(x), axis=1)\n",
        "podcasts_df=podcasts_df[['book_title','text']]\n",
        "podcasts_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>book_title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Clara Callan</td>\n",
              "      <td>Clara Callan Richard Bruce Wright HarperFlamin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
              "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What If?: The World's Foremost Military Histor...</td>\n",
              "      <td>What If?: The World's Foremost Military Histor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PLEADING GUILTY</td>\n",
              "      <td>PLEADING GUILTY Scott Turow Audioworks 9 9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beloved (Plume Contemporary Fiction)</td>\n",
              "      <td>Beloved (Plume Contemporary Fiction) Toni Morr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          book_title                                               text\n",
              "0                                       Clara Callan  Clara Callan Richard Bruce Wright HarperFlamin...\n",
              "1  Flu: The Story of the Great Influenza Pandemic...  Flu: The Story of the Great Influenza Pandemic...\n",
              "2  What If?: The World's Foremost Military Histor...  What If?: The World's Foremost Military Histor...\n",
              "3                                    PLEADING GUILTY         PLEADING GUILTY Scott Turow Audioworks 9 9\n",
              "4               Beloved (Plume Contemporary Fiction)  Beloved (Plume Contemporary Fiction) Toni Morr..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-6HQKTiU01L",
        "outputId": "dc2bcb4b-4a20-4209-bb90-414c43cecc7c"
      },
      "source": [
        "podcasts_df['text'].sample(1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "889    Die Nadel Ken Follett Gustav Lubbe Verlag GmbH...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAZPwnQST-Z5"
      },
      "source": [
        "# create list of stop words\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# remove non-alphanumeric, non-space\n",
        "stop = [re.sub(r'([^\\s\\w]|_)+', '', x) for x in stop]\n",
        "\n",
        "def topKFrequent(tokenized_text, k): \n",
        "   \n",
        "    count = Counter(tokenized_text)   \n",
        "    \n",
        "    return heapq.nlargest(k, count.keys(), key=count.get)\n",
        "\n",
        "def remove_stop(text, stop):\n",
        "    custom_stop = stop\n",
        "#     top5 = topKFrequent(text, 5)\n",
        "#     custom_stop = custom_stop + top5\n",
        "    \n",
        "    new_text = []\n",
        "    for word in text:\n",
        "        if word not in custom_stop:\n",
        "            new_text.append(word)\n",
        "    return new_text\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# create stemmer\n",
        "p_stemmer = PorterStemmer()\n",
        "l_stemmer = WordNetLemmatizer() \n",
        "\n",
        "\n",
        "def stem_list(text, p_stemmer):\n",
        "    new_list = []\n",
        "    for word in text:\n",
        "        new_list.append(p_stemmer.stem(word))\n",
        "    return new_list\n",
        "\n",
        "def lem_list(text, l_stemmer):\n",
        "    new_list = []\n",
        "    for word in text:\n",
        "        new_list.append(l_stemmer.lemmatize(word))\n",
        "    return new_list\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # remove mixed alphanumeric\n",
        "    text = re.sub(r\"\"\"(?x) # verbose regex\n",
        "                            \\b    # Start of word\n",
        "                            (?=   # Look ahead to ensure that this word contains...\n",
        "                             \\w*  # (after any number of alphanumeric characters)\n",
        "                             \\d   # ...at least one digit.\n",
        "                            )     # End of lookahead\n",
        "                            \\w+   # Match the alphanumeric word\n",
        "                            \\s*   # Match any following whitespace\"\"\", \n",
        "                             \"\", text)\n",
        "    \n",
        "    # remove urls (will check and remove http and www later)\n",
        "    text = re.sub(r'\\s([\\S]*.com[\\S]*)\\b', '', text)\n",
        "    text = re.sub(r'\\s([\\S]*.org[\\S]*)\\b', '', text)\n",
        "    text = re.sub(r'\\s([\\S]*.net[\\S]*)\\b', '', text)\n",
        "    text = re.sub(r'\\s([\\S]*.edu[\\S]*)\\b', '', text)\n",
        "    text = re.sub(r'\\s([\\S]*.gov[\\S]*)\\b', '', text)\n",
        "    \n",
        "    # remove non-alphanumeric, non-space\n",
        "    text = re.sub(r'([^\\s\\w]|_)+', '', text)\n",
        "    \n",
        "    # tokenize text\n",
        "    text = tokenizer.tokenize(text.lower())\n",
        "    \n",
        "    # remove stop words\n",
        "    text = remove_stop(text, stop)\n",
        "    \n",
        "    # stem\n",
        "    text = lem_list(text, l_stemmer)\n",
        "    \n",
        "    # remove instances of http or www\n",
        "    new_text_list = []\n",
        "    for word in text:\n",
        "        if re.search(r'http', word):\n",
        "            continue\n",
        "        if re.search(r'www', word):\n",
        "            continue\n",
        "        new_text_list.append(word)\n",
        "    \n",
        "    new_text = ' '.join(new_text_list)\n",
        "    \n",
        "    return new_text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teMblNO-WJNk"
      },
      "source": [
        "podcasts_df['text'] = podcasts_df['text'].map(preprocess_text)\n",
        "podcasts_df = podcasts_df[podcasts_df.text != '']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "kEb4FT9UaOxz",
        "outputId": "e938787a-b5f3-4c34-e044-435b033986ad"
      },
      "source": [
        "podcasts_df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>book_title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Clara Callan</td>\n",
              "      <td>clara callan richard bruce wright harperflamin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
              "      <td>flu story great influenza pandemic search viru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What If?: The World's Foremost Military Histor...</td>\n",
              "      <td>world foremost military historian imagine migh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PLEADING GUILTY</td>\n",
              "      <td>pleading guilty scott turow audioworks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Beloved (Plume Contemporary Fiction)</td>\n",
              "      <td>beloved plume contemporary fiction toni morris...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8995</th>\n",
              "      <td>Flame Of Recca (Flame Of Recca)</td>\n",
              "      <td>flame recca flame recca nobuyuki anzai viz com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8996</th>\n",
              "      <td>Love Hina (Book 3)</td>\n",
              "      <td>love hina book ken akamatsu tokyopop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8997</th>\n",
              "      <td>Real Bout High School, Book 3</td>\n",
              "      <td>real bout high school book reiji saiga tokyopop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8998</th>\n",
              "      <td>Real Bout High School, Book 4</td>\n",
              "      <td>real bout high school book reiji saiga tokyopo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8999</th>\n",
              "      <td>GTO #4</td>\n",
              "      <td>gto tohru fujisawa tokyopop</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             book_title                                               text\n",
              "0                                          Clara Callan  clara callan richard bruce wright harperflamin...\n",
              "1     Flu: The Story of the Great Influenza Pandemic...  flu story great influenza pandemic search viru...\n",
              "2     What If?: The World's Foremost Military Histor...  world foremost military historian imagine migh...\n",
              "3                                       PLEADING GUILTY             pleading guilty scott turow audioworks\n",
              "4                  Beloved (Plume Contemporary Fiction)  beloved plume contemporary fiction toni morris...\n",
              "...                                                 ...                                                ...\n",
              "8995                    Flame Of Recca (Flame Of Recca)  flame recca flame recca nobuyuki anzai viz com...\n",
              "8996                                 Love Hina (Book 3)               love hina book ken akamatsu tokyopop\n",
              "8997                      Real Bout High School, Book 3    real bout high school book reiji saiga tokyopop\n",
              "8998                      Real Bout High School, Book 4  real bout high school book reiji saiga tokyopo...\n",
              "8999                                             GTO #4                        gto tohru fujisawa tokyopop\n",
              "\n",
              "[9000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEuSZwm7WQ2y"
      },
      "source": [
        "def get_recommendations(title, sim_matrix):\n",
        "    index=podcasts_df[podcasts_df['book_title']==title].index[0]\n",
        "    distances=sorted(list(enumerate(sim_matrix[index])),reverse=True,key=lambda x:x[1])\n",
        "    for i in distances[1:6]:\n",
        "      print(podcasts_df.iloc[i[0]].book_title)                 \n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE8NYe0NSbi9"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cv = CountVectorizer(max_features=5000,stop_words='english')\n",
        "cv_matrix = cv.fit_transform(podcasts_df[\"text\"]).toarray()\n",
        "cv_cosine_sim = cosine_similarity(cv_matrix)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q8Fh9z7dybP",
        "outputId": "a532f072-a251-4abb-f9dd-ad5106ecd9c8"
      },
      "source": [
        "get_recommendations('Beloved (Plume Contemporary Fiction)', cv_cosine_sim)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How the Garcia Girls Lost Their Accents (Plume Contemporary Fiction)\n",
            "A Wild Sheep Chase: A Novel (Plume Contemporary Fiction)\n",
            "Black Water (Plume Contemporary Fiction)\n",
            "Story of O\n",
            "Chang and Eng: A Novel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DQGv-YRSDNZ"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf = TfidfVectorizer()\n",
        "tf_matrix = tf.fit_transform(podcasts_df[\"text\"])\n",
        "tf_cosine_sim = cosine_similarity(tf_matrix)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DWDdDnEkdTP",
        "outputId": "066792cc-10b5-4315-a868-15d114fc2efd"
      },
      "source": [
        "get_recommendations('Beloved (Plume Contemporary Fiction)', tf_cosine_sim)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beloved : Gift Edition\n",
            "How the Garcia Girls Lost Their Accents (Plume Contemporary Fiction)\n",
            "Song of Solomon (Oprah's Book Club (Paperback))\n",
            "A Wild Sheep Chase: A Novel (Plume Contemporary Fiction)\n",
            "Wilderness : The Lost Writings of Jim Morrison (Morrison, Jim, Lost Writings of Jim Morrison (Vintage Books (Firm)), V. 1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbOT5V91khvV"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9dvfBrrktcM"
      },
      "source": [
        "class MyTokenizer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        transformed_X = []\n",
        "        for document in X:\n",
        "            tokenized_doc = []\n",
        "            for sent in nltk.sent_tokenize(document):\n",
        "                tokenized_doc += nltk.word_tokenize(sent)\n",
        "            transformed_X.append(np.array(tokenized_doc))\n",
        "        return np.array(transformed_X)\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.transform(X)\n",
        "\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(word2vec.wv.syn0[0])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = MyTokenizer().fit_transform(X)\n",
        "        \n",
        "        return np.array([\n",
        "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.transform(X)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KPq3w7ok6hv"
      },
      "source": [
        "\n",
        "text_list = list(podcasts_df.text)\n",
        "tokenized_text = [tokenizer.tokenize(i) for i in text_list]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEa_brO9k-Ow"
      },
      "source": [
        "w2v_model = Word2Vec(tokenized_text, sg=1)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz1MsqLKlAQv",
        "outputId": "bca02605-b9ce-48c2-fca0-49179709703a"
      },
      "source": [
        "nltk.download('punkt')\n",
        "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2v_model)\n",
        "mean_embedded = mean_embedding_vectorizer.fit_transform(podcasts_df['text'])\n",
        "w2v_cosine_sim = cosine_similarity(mean_embedded)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUIMJLfhlC1J",
        "outputId": "6b9d8cb1-7bec-45d5-9424-8213f3352d56"
      },
      "source": [
        "get_recommendations('Beloved (Plume Contemporary Fiction)', w2v_cosine_sim)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Notebooks of Don Rigoberto\n",
            "Vox: A Novel\n",
            "Oracles and Miracles\n",
            "Joke\n",
            "Midnight's Children\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx4G5y6flNef"
      },
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc7V87-plb-A"
      },
      "source": [
        "def remove_first_principal_component(X):\n",
        "    svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)\n",
        "    svd.fit(X)\n",
        "    pc = svd.components_\n",
        "    XX = X - X.dot(pc.transpose()) * pc\n",
        "    return XX\n",
        "\n",
        "def smooth_inverse_frequency(sent, a=0.001, word2vec_model=w2v_model):\n",
        "    word_counter = {}\n",
        "    sentences = []\n",
        "    total_count = 0\n",
        "    no_of_sentences = 0\n",
        "    \n",
        "    for s in sent:\n",
        "        for w in s:\n",
        "            if w in word_counter:\n",
        "                word_counter[w] = word_counter[w] + 1\n",
        "            else:\n",
        "                word_counter[w] = 1\n",
        "        total_count = total_count + len(s)\n",
        "        no_of_sentences = no_of_sentences + 1\n",
        "    \n",
        "    sents_emd = []\n",
        "    for s in sent:\n",
        "        sent_emd = []\n",
        "        for word in s:\n",
        "            if word in word2vec_model:\n",
        "                emd = (a/(a + (word_counter[word]/total_count)))*word2vec_model[word]\n",
        "                sent_emd.append(emd)\n",
        "        sum_ = np.array(sent_emd).sum(axis=0)\n",
        "        sentence_emd = sum_/float(no_of_sentences)\n",
        "        sents_emd.append(sentence_emd)\n",
        "        \n",
        "    new_sents_emb = remove_first_principal_component(np.array(sents_emd))\n",
        "    return new_sents_emb"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM55vhL3lghC",
        "outputId": "2e842f6d-07e7-4bb8-fd85-823b161406af"
      },
      "source": [
        "sif_text_emd = smooth_inverse_frequency(text_list)\n",
        "sif_cosine_sim = cosine_similarity(sif_text_emd)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6_U8ECQlkED",
        "outputId": "7c7416d5-5f65-4ec2-801b-548d605fb932"
      },
      "source": [
        "get_recommendations('Beloved (Plume Contemporary Fiction)', sif_cosine_sim)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Man, Woman, and the Meaning of Love: God's Plan for Love, Marriage, Intimacy, and the Family\n",
            "Flatland: A Romance of Many Dimensions (Dover Thrift Editions)\n",
            "Confucio nel computer: Memoria accidentale del futuro\n",
            "The Nightspinners : A Novel\n",
            "Three Lives: Stories of the Good Anna, Melanctha and the Gentle Lena (Dover Thrift Editions)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcE-q2DA3svM"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqw0eVUiE995"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}