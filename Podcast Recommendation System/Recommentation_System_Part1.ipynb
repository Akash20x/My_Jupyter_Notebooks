{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommentation System-Part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjgmHdcfK7mI",
        "outputId": "4ee9a0c5-e31e-435a-f043-f4e6724b34ea"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import heapq\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIRFvGE44tcH"
      },
      "source": [
        "# Dataset Link:\n",
        "* https://www.kaggle.com/listennotes/all-podcast-episodes-published-in-december-2017?select=podcasts.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OFM0OemS9fz"
      },
      "source": [
        "podcasts_df = pd.read_csv('/content/podcasts.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "pPQ_7rHOUBW7",
        "outputId": "4b2bd866-35c7-46f2-cbc4-d0c7ea695039"
      },
      "source": [
        "podcasts_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>title</th>\n",
              "      <th>image</th>\n",
              "      <th>description</th>\n",
              "      <th>language</th>\n",
              "      <th>categories</th>\n",
              "      <th>website</th>\n",
              "      <th>author</th>\n",
              "      <th>itunes_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8d62d3880db2425b890b986e58aca393</td>\n",
              "      <td>Ecommerce Conversations, by Practical Ecommerce</td>\n",
              "      <td>http://is4.mzstatic.com/image/thumb/Music6/v4/...</td>\n",
              "      <td>Listen in as the Practical Ecommerce editorial...</td>\n",
              "      <td>English</td>\n",
              "      <td>Technology</td>\n",
              "      <td>http://www.practicalecommerce.com</td>\n",
              "      <td>Practical Ecommerce</td>\n",
              "      <td>874457373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cbbefd691915468c90f87ab2f00473f9</td>\n",
              "      <td>Eat Sleep Code Podcast</td>\n",
              "      <td>http://is4.mzstatic.com/image/thumb/Music71/v4...</td>\n",
              "      <td>On the show weâ€™ll be talking to passionate peo...</td>\n",
              "      <td>English</td>\n",
              "      <td>Tech News | Technology</td>\n",
              "      <td>http://developer.telerik.com/</td>\n",
              "      <td>Telerik</td>\n",
              "      <td>1015556393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>73626ad1edb74dbb8112cd159bda86cf</td>\n",
              "      <td>SoundtrackAlley</td>\n",
              "      <td>http://is5.mzstatic.com/image/thumb/Music71/v4...</td>\n",
              "      <td>A podcast about soundtracks and movies from my...</td>\n",
              "      <td>English</td>\n",
              "      <td>Podcasting | Technology</td>\n",
              "      <td>https://soundtrackalley.podbean.com</td>\n",
              "      <td>Randy Andrews</td>\n",
              "      <td>1158188937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0f50631ebad24cedb2fee80950f37a1a</td>\n",
              "      <td>The Tech M&amp;A Podcast</td>\n",
              "      <td>http://is1.mzstatic.com/image/thumb/Music71/v4...</td>\n",
              "      <td>The Tech M&amp;A Podcast pulls from the best of th...</td>\n",
              "      <td>English</td>\n",
              "      <td>Business News | Technology | Tech News | Business</td>\n",
              "      <td>http://www.corumgroup.com</td>\n",
              "      <td>Timothy Goddard</td>\n",
              "      <td>538160025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69580e7b419045839ca07af06cf0d653</td>\n",
              "      <td>The Tech Informist - For fans of Apple, Google...</td>\n",
              "      <td>http://is4.mzstatic.com/image/thumb/Music62/v4...</td>\n",
              "      <td>The tech news show with two guys shooting the ...</td>\n",
              "      <td>English</td>\n",
              "      <td>Gadgets | Tech News | Technology</td>\n",
              "      <td>http://techinformist.com</td>\n",
              "      <td>The Tech Informist</td>\n",
              "      <td>916080498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               uuid  ...   itunes_id\n",
              "0  8d62d3880db2425b890b986e58aca393  ...   874457373\n",
              "1  cbbefd691915468c90f87ab2f00473f9  ...  1015556393\n",
              "2  73626ad1edb74dbb8112cd159bda86cf  ...  1158188937\n",
              "3  0f50631ebad24cedb2fee80950f37a1a  ...   538160025\n",
              "4  69580e7b419045839ca07af06cf0d653  ...   916080498\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL4mtVweYYfD",
        "outputId": "8c9216b4-7275-40a4-92b2-6e5780c594a9"
      },
      "source": [
        "podcasts_df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 121175 entries, 0 to 121174\n",
            "Data columns (total 9 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   uuid         121175 non-null  object\n",
            " 1   title        121173 non-null  object\n",
            " 2   image        121175 non-null  object\n",
            " 3   description  119832 non-null  object\n",
            " 4   language     121175 non-null  object\n",
            " 5   categories   121175 non-null  object\n",
            " 6   website      120005 non-null  object\n",
            " 7   author       118678 non-null  object\n",
            " 8   itunes_id    121175 non-null  int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 8.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OmXvbh_YzMi"
      },
      "source": [
        "\n",
        "podcasts_df.dropna(inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pISPQaKYipfj"
      },
      "source": [
        "podcasts_df=podcasts_df.iloc[0:9000].reset_index()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ZwslEjdwXI1k",
        "outputId": "eaae98ef-276a-4ca1-c833-dd9a8c1b7cb3"
      },
      "source": [
        "podcasts_df['text'] = podcasts_df[['title', 'author', 'categories', 'description']].apply(lambda x: \" \".join(x), axis=1)\n",
        "podcasts_df=podcasts_df[['title','text']]\n",
        "podcasts_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ecommerce Conversations, by Practical Ecommerce</td>\n",
              "      <td>Ecommerce Conversations, by Practical Ecommerc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Eat Sleep Code Podcast</td>\n",
              "      <td>Eat Sleep Code Podcast Telerik Tech News | Tec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SoundtrackAlley</td>\n",
              "      <td>SoundtrackAlley Randy Andrews Podcasting | Tec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Tech M&amp;A Podcast</td>\n",
              "      <td>The Tech M&amp;A Podcast Timothy Goddard Business ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Tech Informist - For fans of Apple, Google...</td>\n",
              "      <td>The Tech Informist - For fans of Apple, Google...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title                                               text\n",
              "0    Ecommerce Conversations, by Practical Ecommerce  Ecommerce Conversations, by Practical Ecommerc...\n",
              "1                             Eat Sleep Code Podcast  Eat Sleep Code Podcast Telerik Tech News | Tec...\n",
              "2                                    SoundtrackAlley  SoundtrackAlley Randy Andrews Podcasting | Tec...\n",
              "3                               The Tech M&A Podcast  The Tech M&A Podcast Timothy Goddard Business ...\n",
              "4  The Tech Informist - For fans of Apple, Google...  The Tech Informist - For fans of Apple, Google..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "a-6HQKTiU01L",
        "outputId": "19394fb0-5b91-476b-8e8a-756e9e09d3f0"
      },
      "source": [
        "podcasts_df['text'][0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ecommerce Conversations, by Practical Ecommerce Practical Ecommerce Technology Listen in as the Practical Ecommerce editorial staff interviews interesting personalities in the ecommerce space.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAZPwnQST-Z5"
      },
      "source": [
        "# create list of stop words\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# remove non-alphanumeric, non-space\n",
        "stop = [re.sub(r'([^\\s\\w]|_)+', '', x) for x in stop]\n",
        "\n",
        "# add in custom stop words\n",
        "days = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
        "\n",
        "months = ['january', 'february', 'march', 'april', 'may', 'june', \n",
        "          'july', 'august', 'september', 'october', 'november', 'december']\n",
        "\n",
        "other = ['nan', 'podcast', 'podcasts', 'every', 'new', 'weekly', \n",
        "         'stories', 'story', 'episode', 'episodes', 'listen', \n",
        "         'host', 'hosted', 'join']\n",
        "\n",
        "[stop.append(str(day)) for day in days]\n",
        "[stop.append(str(month)) for month in months]\n",
        "[stop.append(str(x)) for x in other]\n",
        "\n",
        "def topKFrequent(tokenized_text, k): \n",
        "   \n",
        "    count = Counter(tokenized_text)   \n",
        "    \n",
        "    return heapq.nlargest(k, count.keys(), key=count.get)\n",
        "\n",
        "def remove_stop(text, stop):\n",
        "    custom_stop = stop\n",
        "#     top5 = topKFrequent(text, 5)\n",
        "#     custom_stop = custom_stop + top5\n",
        "    \n",
        "    new_text = []\n",
        "    for word in text:\n",
        "        if word not in custom_stop:\n",
        "            new_text.append(word)\n",
        "    return new_text\n",
        "\n",
        "# create tokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# create stemmer\n",
        "p_stemmer = PorterStemmer()\n",
        "l_stemmer = WordNetLemmatizer() \n",
        "\n",
        "\n",
        "def stem_list(text, p_stemmer):\n",
        "    new_list = []\n",
        "    for word in text:\n",
        "        new_list.append(p_stemmer.stem(word))\n",
        "    return new_list\n",
        "\n",
        "def lem_list(text, l_stemmer):\n",
        "    new_list = []\n",
        "    for word in text:\n",
        "        new_list.append(l_stemmer.lemmatize(word))\n",
        "    return new_list\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # remove mixed alphanumeric\n",
        "    text = re.sub(r\"\"\"(?x) # verbose regex\n",
        "                            \\b    # Start of word\n",
        "                            (?=   # Look ahead to ensure that this word contains...\n",
        "                             \\w*  # (after any number of alphanumeric characters)\n",
        "                             \\d   # ...at least one digit.\n",
        "                            )     # End of lookahead\n",
        "                            \\w+   # Match the alphanumeric word\n",
        "                            \\s*   # Match any following whitespace\"\"\", \n",
        "                             \"\", text)\n",
        "    \n",
        "    # remove urls (will check and remove http and www later)\n",
        "    text = re.sub(r'\\s([\\S]*.com[\\S]*)\\b', '', text)\n",
        "    text = re.sub(r'\\s([\\S]*.org[\\S]*)\\b', '', text)\n",
        "    text = re.sub(r'\\s([\\S]*.net[\\S]*)\\b', '', text)\n",
        "    text = re.sub(r'\\s([\\S]*.edu[\\S]*)\\b', '', text)\n",
        "    text = re.sub(r'\\s([\\S]*.gov[\\S]*)\\b', '', text)\n",
        "    \n",
        "    # remove non-alphanumeric, non-space\n",
        "    text = re.sub(r'([^\\s\\w]|_)+', '', text)\n",
        "    \n",
        "    # tokenize text\n",
        "    text = tokenizer.tokenize(text.lower())\n",
        "    \n",
        "    # remove stop words\n",
        "    text = remove_stop(text, stop)\n",
        "    \n",
        "    # stem\n",
        "    text = lem_list(text, l_stemmer)\n",
        "    \n",
        "    # remove instances of http or www\n",
        "    new_text_list = []\n",
        "    for word in text:\n",
        "        if re.search(r'http', word):\n",
        "            continue\n",
        "        if re.search(r'www', word):\n",
        "            continue\n",
        "        new_text_list.append(word)\n",
        "    \n",
        "    new_text = ' '.join(new_text_list)\n",
        "    \n",
        "    return new_text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teMblNO-WJNk"
      },
      "source": [
        "podcasts_df['text'] = podcasts_df['text'].map(preprocess_text)\n",
        "podcasts_df = podcasts_df[podcasts_df.text != '']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "kEb4FT9UaOxz",
        "outputId": "454dddbe-a63b-49e7-9220-1556336d144c"
      },
      "source": [
        "podcasts_df"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ecommerce Conversations, by Practical Ecommerce</td>\n",
              "      <td>ecommerce conversation practical practical tec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Eat Sleep Code Podcast</td>\n",
              "      <td>eat sleep code telerik tech news technology sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SoundtrackAlley</td>\n",
              "      <td>soundtrackalley randy andrew podcasting techno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Tech M&amp;A Podcast</td>\n",
              "      <td>tech timothy goddard business news technology ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Tech Informist - For fans of Apple, Google...</td>\n",
              "      <td>tech informist fan apple google microsoft tech...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8995</th>\n",
              "      <td>HHWLOD Master Feed</td>\n",
              "      <td>hhwlod master feed hhwlod network literature t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8996</th>\n",
              "      <td>Horror Movie Night</td>\n",
              "      <td>horror movie night horror movie night comedy t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8997</th>\n",
              "      <td>Hey Let's Talk About That Movie or Whatever</td>\n",
              "      <td>hey let talk movie whatever hey let talk movie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8998</th>\n",
              "      <td>Let's Get Animated!</td>\n",
              "      <td>let get animated let get animated tv film kick...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8999</th>\n",
              "      <td>LWAF</td>\n",
              "      <td>lwaf joe vallero art tv film visual art lwaf s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title                                               text\n",
              "0       Ecommerce Conversations, by Practical Ecommerce  ecommerce conversation practical practical tec...\n",
              "1                                Eat Sleep Code Podcast  eat sleep code telerik tech news technology sh...\n",
              "2                                       SoundtrackAlley  soundtrackalley randy andrew podcasting techno...\n",
              "3                                  The Tech M&A Podcast  tech timothy goddard business news technology ...\n",
              "4     The Tech Informist - For fans of Apple, Google...  tech informist fan apple google microsoft tech...\n",
              "...                                                 ...                                                ...\n",
              "8995                                 HHWLOD Master Feed  hhwlod master feed hhwlod network literature t...\n",
              "8996                                 Horror Movie Night  horror movie night horror movie night comedy t...\n",
              "8997        Hey Let's Talk About That Movie or Whatever  hey let talk movie whatever hey let talk movie...\n",
              "8998                                Let's Get Animated!  let get animated let get animated tv film kick...\n",
              "8999                                               LWAF  lwaf joe vallero art tv film visual art lwaf s...\n",
              "\n",
              "[9000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEuSZwm7WQ2y"
      },
      "source": [
        "def get_recommendations(title, sim_matrix):\n",
        "    index=podcasts_df[podcasts_df['title']==title].index[0]\n",
        "    distances=sorted(list(enumerate(sim_matrix[index])),reverse=True,key=lambda x:x[1])\n",
        "    for i in distances[1:6]:\n",
        "      print(podcasts_df.iloc[i[0]].title)                 \n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE8NYe0NSbi9"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cv = CountVectorizer(max_features=5000,stop_words='english')\n",
        "cv_matrix = cv.fit_transform(podcasts_df[\"text\"]).toarray()\n",
        "cv_cosine_sim = cosine_similarity(cv_matrix)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q8Fh9z7dybP",
        "outputId": "fbbcdb12-21a5-420f-d546-9fe6e8e784e0"
      },
      "source": [
        "get_recommendations('Horror Movie Night', cv_cosine_sim)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Terrible Terror Podcast\n",
            "Movies Unhacked\n",
            "Horrible Horror Podcast\n",
            "Movie Improvie\n",
            "Something Cheeky: Movies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DQGv-YRSDNZ"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf = TfidfVectorizer()\n",
        "tf_matrix = tf.fit_transform(podcasts_df[\"text\"])\n",
        "tf_cosine_sim = cosine_similarity(tf_matrix)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DWDdDnEkdTP",
        "outputId": "921191fc-fab5-4c67-fd17-44b66929a07d"
      },
      "source": [
        "get_recommendations('Horror Movie Night', tf_cosine_sim)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "United Nations of Horror\n",
            "Horrible Horror Podcast\n",
            "Music City Horror Podcast\n",
            "Horrified Chicken\n",
            "Nightmare On Film Street - A Horror Movie Podcast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbOT5V91khvV"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9dvfBrrktcM"
      },
      "source": [
        "class MyTokenizer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        transformed_X = []\n",
        "        for document in X:\n",
        "            tokenized_doc = []\n",
        "            for sent in nltk.sent_tokenize(document):\n",
        "                tokenized_doc += nltk.word_tokenize(sent)\n",
        "            transformed_X.append(np.array(tokenized_doc))\n",
        "        return np.array(transformed_X)\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.transform(X)\n",
        "\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(word2vec.wv.syn0[0])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = MyTokenizer().fit_transform(X)\n",
        "        \n",
        "        return np.array([\n",
        "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.transform(X)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KPq3w7ok6hv"
      },
      "source": [
        "\n",
        "text_list = list(podcasts_df.text)\n",
        "tokenized_text = [tokenizer.tokenize(i) for i in text_list]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEa_brO9k-Ow"
      },
      "source": [
        "w2v_model = Word2Vec(tokenized_text, sg=1)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz1MsqLKlAQv",
        "outputId": "d017f778-1c93-4af9-d36f-4215c5b03db6"
      },
      "source": [
        "nltk.download('punkt')\n",
        "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2v_model)\n",
        "mean_embedded = mean_embedding_vectorizer.fit_transform(podcasts_df['text'])\n",
        "w2v_cosine_sim = cosine_similarity(mean_embedded)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUIMJLfhlC1J",
        "outputId": "a3ab540e-5a61-4576-9a8f-4cf599ac7c2f"
      },
      "source": [
        "get_recommendations('Horror Movie Night', w2v_cosine_sim)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Horrible Horror Podcast\n",
            "Married With Movies\n",
            "Have You Seen This One? (HYSTO?)\n",
            "Rated M for MacPhail - 1001 Movies and Beyond\n",
            "Decades Podcast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx4G5y6flNef"
      },
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc7V87-plb-A"
      },
      "source": [
        "def remove_first_principal_component(X):\n",
        "    svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)\n",
        "    svd.fit(X)\n",
        "    pc = svd.components_\n",
        "    XX = X - X.dot(pc.transpose()) * pc\n",
        "    return XX\n",
        "\n",
        "def smooth_inverse_frequency(sent, a=0.001, word2vec_model=w2v_model):\n",
        "    word_counter = {}\n",
        "    sentences = []\n",
        "    total_count = 0\n",
        "    no_of_sentences = 0\n",
        "    \n",
        "    for s in sent:\n",
        "        for w in s:\n",
        "            if w in word_counter:\n",
        "                word_counter[w] = word_counter[w] + 1\n",
        "            else:\n",
        "                word_counter[w] = 1\n",
        "        total_count = total_count + len(s)\n",
        "        no_of_sentences = no_of_sentences + 1\n",
        "    \n",
        "    sents_emd = []\n",
        "    for s in sent:\n",
        "        sent_emd = []\n",
        "        for word in s:\n",
        "            if word in word2vec_model:\n",
        "                emd = (a/(a + (word_counter[word]/total_count)))*word2vec_model[word]\n",
        "                sent_emd.append(emd)\n",
        "        sum_ = np.array(sent_emd).sum(axis=0)\n",
        "        sentence_emd = sum_/float(no_of_sentences)\n",
        "        sents_emd.append(sentence_emd)\n",
        "        \n",
        "    new_sents_emb = remove_first_principal_component(np.array(sents_emd))\n",
        "    return new_sents_emb"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM55vhL3lghC",
        "outputId": "ad2c3ac8-e52b-471c-ffc8-eb753a651ce5"
      },
      "source": [
        "sif_text_emd = smooth_inverse_frequency(text_list)\n",
        "sif_cosine_sim = cosine_similarity(sif_text_emd)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6_U8ECQlkED",
        "outputId": "1a355ba3-0415-4939-baca-9555eac99bad"
      },
      "source": [
        "get_recommendations('Horror Movie Night', sif_cosine_sim)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TransMissions - Everything Transformers!\n",
            "Gold Ave Church Sermons\n",
            "Trinity Evangelical Church\n",
            "Freethought Radio\n",
            "THE WEEKLY MOTOR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcE-q2DA3svM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}